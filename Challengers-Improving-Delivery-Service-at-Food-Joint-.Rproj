# Install and load required libraries
install.packages("dplyr")
library(dplyr)

install.packages("ggplot2")

# Read data from CSV file
data <- read.csv("data/finalTrain.csv")

# Check first few rows of the data
head(data, 3)

# Check rows with missing values in "Delivery_person_Age"
head(data[is.na(data$Delivery_person_Age)], 3)

# Check the data types and number of non-null values for each column
summary(data)

# Drop columns with no information or unique values
data <- data %>% select(-Delivery_person_ID, -Delivery_person_ID, -ID, -Time_Orderd, -Time_Order_picked)

# Check the data types and number of non-null values for each column after dropping columns
summary(data)

# Check first few rows of the data after dropping columns
head(data, 3)

# Import required libraries
library(matplotlib.pyplot)
library(seaborn)

# Define a function to plot histograms with density curves
histplotcount_func <- function(input_feature, data, no_of_bins) {
  ggplot(data, aes(x = input_feature)) +
    geom_histogram(bins = no_of_bins) +
    stat_density(geom = "line", color = "blue") +
    labs(title = paste("Distribution of", input_feature), x = input_feature, y = "Frequency") +
    theme_bw()

  # Show the plot
  print(ggplot2::last_plot())
}

# Plot histogram with density curve for "Delivery_person_Age"
histplotcount_func("Delivery_person_Age", data, 5)

# Calculate the mean of "Delivery_person_Age" to fill missing values
mean_age <- mean(data$Delivery_person_Age)

# Fill missing values in "Delivery_person_Age" with the mean
data$Delivery_person_Age <- replace(data$Delivery_person_Age, is.na(data$Delivery_person_Age), mean_age)

# Check the distribution of "Delivery_person_Ratings"
data$Delivery_person_Ratings %>% count()

# Remove rows where "Delivery_person_Ratings" is 6
data <- data %>% filter(Delivery_person_Ratings != 6)

# Plot histogram with density curve for "Delivery_person_Ratings"
histplotcount_func("Delivery_person_Ratings", data, 5)

# Loop through each column in the DataFrame and print information about each column
for (i in colnames(data)) {
  print(paste0("Column Name: ", i))
  print(paste0("Column Type: ", typeof(data[[i]])))
  print(rep("=", 64))

  # Print unique values in the column
  unique_values <- unique(data[[i]])
  print(unique_values)
  print(rep("=", 64))
}

# Count the number of records with zero values in "Delivery_location_latitude"
count_zero_longitude <- sum(data$Delivery_location_latitude == "0")
print(paste0("Number of records with Delivery_location_latitude equal to 0: ", count_zero_longitude))

# Count the number of records with zero values in "Restaurant_latitude"
count_zero_latitude <- sum(data$Restaurant_latitude == 0)
print(paste0("Number of records with Restaurant_latitude equal to 0: ", count_zero_latitude))

# Count the number of records with zero values in "Restaurant_longitude"
count_zero_longitude <- sum(data$Restaurant_longitude == 0)
print(paste0("Number of records with Restaurant_longitude equal to 0: ", count_zero_longitude))

# Plot histogram with density curve for "Restaurant_latitude"
histplotcount_func("Restaurant_latitude", data, 5)

# Plot histogram with density curve for "Restaurant_longitude"
histplotcount_func("Restaurant_longitude", data, 5)

# Check the data types and number of non-null values for each column again
summary(data)

# Calculate the median of "Delivery_location_longitude" to fill missing values
median_Delivery_location_longitude <- median(data$Delivery_location_longitude)

# Fill missing values in "Delivery_location_longitude" with the median
data$Delivery_location_longitude <- replace(data$Delivery_location_longitude, is.na(data$Delivery_location_longitude), median_Delivery_location_longitude)

# Calculate the median of "Delivery_location_latitude" to fill missing values
median_Delivery_location_latitude <- median(data$Delivery_location_latitude)

# Fill missing values in "Delivery_location_latitude" with the median
data$Delivery_location_latitude <- replace(data$Delivery_location_latitude, is.na(data$Delivery_location_latitude), median_Delivery_location_latitude)

# Check for duplicate rows
duplicate_rows <- data[duplicated(data)]

# Check the number of rows with and without duplicates
data.shape

# Calculate the percentage of rows with duplicates
percentage_duplicates <- (data.shape[0] - without_nan[0]) / data.shape[0] * 100
print(paste0("Percentage of rows with duplicates: ", round(percentage_duplicates, 2), "%"))

# Calculate the percentage of missing values in the dataset
missing_values_percentage <- (with_nan[0] - without_nan[0]) / with_nan[0] * 100
print(paste0("Percentage of missing values in the dataset: ", round(missing_values_percentage, 2), "%"))

# Calculate the percentage of valid data in the dataset
valid_data_percentage <- 100 - missing_values_percentage
print(paste0("Percentage of valid data in the dataset: ", round(valid_data_percentage, 2), "%"))

# Calculate the conversion rate
conversion_rate <- (2938 / 42646) * 100
print(paste0("Conversion rate: ", round(conversion_rate, 2), "%"))

# Check the data types and number of non-null values for each column again
summary(data)

# Check the first few rows of the "Order_Date" column
data[['Order_Date']].head()

# Check the unique values in "Order_Date"
unique(data$Order_Date)

# Extract the day from the "Order_Date" column and store it in a new column named "Day"
data["Day"] <- as.Date(data$Order_Date, "%d-%m-%Y") %>% ISOdate() %>% as.numeric()

# Check the first few rows of the data after extracting the day
data.head()

# Extract the month from the "Order_Date" column and store it in a new column named "Month"
data["Month"] <- as.Date(data$Order_Date, "%d-%m-%Y") %>% month()

# Extract the year from the "Order_Date" column and store it in a new column named "Year"
data["Year"] <- as.Date(data$Order_Date, "%d-%m-%Y") %>% year()

# Check the first few rows of the data after extracting the month and year
data[["Order_Date"]].head()

# Drop the "Order_Date" column since the information has been extracted into separate columns
data <- data %>% select(-Order_Date)

# Check the first few rows of the data after dropping the "Order_Date" column
data.head(3)

# Check the data types and number of non-null values for each column again
summary(data)

# Rename columns for better readability
names(data) <- c("Delivery_person_ID", "Delivery_person_Ratings", "multiple_deliveries",
                  "Restaurant_ID", "Restaurant_latitude", "Restaurant_longitude",
                  "Delivery_location_latitude", "Delivery_location_longitude",
                  "Road_traffic_density", "Weather_conditions", "Festival", "City",
                  "Day_Ordered", "Month_Ordered", "Year_Ordered", "Time_taken_min")

# Define a function to segregate categorical and numerical columns
column_segregator <- function(data, categorical_columns_list, numerical_columns_list) {
  for (col in colnames(data)) {
    if (is.character(data[[col]])) {
      categorical_columns_list <- append(categorical_columns_list, col)
    } else {
      numerical_columns_list <- append(numerical_columns_list, col)
    }
  }
}

# Initialize empty lists to store categorical and numerical columns
categorical_columns <- list()
numerical_columns <- list()

# Segregate categorical and numerical columns using the custom function
column_segregator(data, categorical_columns, numerical_columns)

# Print the identified categorical and numerical columns
print("Categorical Columns:")
print(categorical_columns)
print("\nNumerical Columns:")
print(numerical_columns)

# Describe the categorical columns using summary()
data[categorical_columns] %>% summary()

# Describe the numerical columns using describe() and transpose the result
data[numerical_columns] %>% describe() %>% t()

# Check for numeric values in categorical columns
for (column in categorical_columns) {
  if (grepl("[0-9]", data[[column]])) {
    print(paste0("Numeric values found in categorical column:", column))
  } else {
    print(paste0("No numeric values found in categorical column:", column))
  }
}

# Filter rows that don't contain ":" (i.e., rows with floating-point values) for "Time_Orderd"
filtered_rows <- data %>% filter(!is.na(Time_Orderd) & !grepl(":", Time_Orderd))

# Sort rows based on "Time_Orderd" column
sorted_rows <- filtered_rows %>% arrange(Time_Orderd)

# Retrieve only the "Time_Orderd" column values
result <- data.frame(Time_Orderd = sorted_rows$Time_Orderd)

# Count the occurrences of each unique value in the resulting column
value_counts <- result$Time_Orderd %>% value_counts()

# Display the value counts
print(value_counts, "\nSum =", sum(value_counts))

# Filter rows that don't contain ":" (i.e., rows with floating-point values) for "Time_Order_picked"
filtered_rows <- data %>% filter(!is.na(Time_Order_picked) & !grepl(":", Time_Order_picked))

# Sort rows based on "Time_Order_picked" column
sorted_rows <- filtered_rows %>% arrange(Time_Order_picked)

# Retrieve only the "Time_Order_picked" column values
result <- data.frame(Time_Order_picked = sorted_rows$Time_Order_picked)

# Count the occurrences of each unique value in the resulting column
value_counts <- result$Time_Order_picked %>% value_counts()

# Display the value counts
print(value_counts, "\nSum =", sum(value_counts))

# Calculate the correlation matrix using only numerical columns
correlation_matrix <- cor(data[numerical_columns], method = "spearman")

# Set up the figure and axes
plot(correlation_matrix, col = colorRamp("coolwarm"), main = "Correlation Heatmap")

# Add a title
text(0, 0.5, "Correlation Matrix", cex = 0.8, adj = c(0.5, 0.5))

# Show the plot
plot(correlation_matrix)

# Print the categorical columns
print("Categorical Columns:")
print(categorical_columns)

# Define a function to create bar plots with standard deviation error bars
barplot_func <- function(data, input_feature, output_feature) {
  ggplot(data, aes_string(x = input_feature, y = output_feature)) +
    geom_bar() +
    geom_errorbar(aes(ymin = output_feature - sd(output_feature), ymax = output_feature + sd(output_feature))) +
    labs(title = paste(output_feature, "by", input_feature), x = input_feature, y = output_feature) +
    theme_bw()

  print("\n")
  print(ggplot2::last_plot())
}

# Create a bar plot of "Time_taken(min)" by "Weather_conditions"
barplot_func(data, "Weather_conditions", "Time_taken_min")

# Get unique values in "Weather_conditions"
unique_weather_conditions <- unique(data$Weather_conditions)

# Create a dictionary to map weather conditions to numerical values
Weather_conditions_map <- c(
  Fog = 1,
  Cloudy = 2,
  Windy = 3,
  Sandstorms = 4,
  Stormy = 5,
  Sunny = 6
)  # (Least likely to cause delays)

# Replace weather conditions with corresponding numerical values
data$Weather_conditions <- sapply(data$Weather_conditions, function(x) Weather_conditions_map[[x]])

# Get unique values in "Road_traffic_density"
unique_traffic_density <- unique(data$Road_traffic_density)

# Create a dictionary to map traffic density to numerical values
Road_traffic_density_map <- c(
  Jam = 1,
  High = 2,
  Medium = 3,
  Low = 4
)

# Replace traffic density with corresponding numerical values
data$Road_traffic_density <- sapply(data$Road_traffic_density, function(x) Road_traffic_density_map[[x]])

# Create a bar plot of "Time_taken(min)" by "Type_of_vehicle"
barplot_func(data, "Type_of_vehicle", "Time_taken_min")

# Get unique values in "Type_of_vehicle"
unique_vehicle_types <- unique(data$Type_of_vehicle)

# Create a dictionary to map vehicle types to numerical values
Type_of_vehicle_map <- c(
  motorcycle = 1,
  scooter = 2,
  electric_scooter = 3
)

# Replace vehicle types with corresponding numerical values
data$Type_of_vehicle <- sapply(data$Type_of_vehicle, function(x) Type_of_vehicle_map[[x]])

# Create a bar plot of "Time_taken(min)" by "City"
barplot_func(data, "City", "Time_taken_min")

# Get unique values in "City"
unique_cities <- unique(data$City)

# Create a dictionary to map city types to numerical values
City_map <- c(
  Urban = 3,
  Metropolitian = 2,
  Semi_Urban = 1
)

# Replace city types with corresponding numerical values
data$City <- sapply(data$City, function(x) City_map[[x]])

# Create a list of the mapping dictionaries' keys
mapping_dicts_keys <- names(mapping_dicts)

# Map values using the mapping dictionaries
for (column in mapping_dicts_keys) {
  data[[column]] <- as.numeric(sapply(data[[column]], function(x) mapping_dicts[[column]][[x]]))
}

# Define a function to perform one-hot encoding for a categorical column in a DataFrame
one_hot_encoder_func <- function(data, column_name) {
  # Validate inputs
  if (!is.data.frame(data)) {
    stop("Input 'data' must be a data.frame.")
  }

  if (!column_name %in% colnames(data)) {
    stop(paste0("Column '", column_name, "' not found in the data.frame."))
  }

  # Perform one-hot encoding
  one_hot_encoded <- data[[column_name]] %>% as.factor() %>% levels() %>% do.call(data.frame, .) %>% setNames(paste0(column_name, "_"))

  # Concatenate the one-hot encoded columns with the original DataFrame
  data <- data %>% cbind(one_hot_encoded)

  # Drop the original column
  data <- data %>% select(-column_name)

  return(data)
}

# Check the first few rows of the data
data[1:3,]

# Get unique values in "Type_of_order"
unique_order_types <- unique(data$Type_of_order)

# Perform one-hot encoding for "Type_of_order"
data <- one_hot_encoder_func(data, "Type_of_order")

# Perform one-hot encoding for "Festival"
data <- one_hot_encoder_func(data, "Festival")

# Check the first few rows of the data after one-hot encoding
data[1:3,]

# Define a function to create a line plot with a KDE curve
lineplot_with_kde <- function(input_feature, output_feature, data) {
  ggplot(data, aes_string(x = input_feature, y = output_feature)) +
    geom_line(color = "blue", label = "Line Plot") +
    geom_density2d(cmap = "Reds", label = "KDE", shade = TRUE) +
    labs(title = paste("Line Plot with KDE of", output_feature, "based on", input_feature), x = input_feature, y = output_feature) +
    theme_bw()

  print("\n")
  print(ggplot2::last_plot())
}

# Target column for line plots
target_col <- "Time_taken_min"

# Create line plots with KDE curves for various features
lineplot_with_kde("Delivery_person_Age", target_col, data)
lineplot_with_kde("Delivery_person_Ratings", target_col, data)
lineplot_with_kde("Weather_conditions", target_col, data)
lineplot_with_kde("Vehicle_condition", target_col, data)

# Print the Weather_conditions_map dictionary
print(Weather_conditions_map)

# Calculate the correlation matrix using only numerical columns
correlation_matrix <- cor(data[numerical_columns], method = "spearman")

# Set up the figure and axes
plot(correlation_matrix, col = colorRamp("coolwarm"), main = "Correlation Heatmap")

# Add a title
text(0, 0.5, "Correlation Matrix", cex = 0.8, adj = c(0.5, 0.5))

# Show the plot
plot(correlation_matrix)

# Create bar plots for additional features
barplot_func(data, "multiple_deliveries", target_col)
barplot_func(data, "Festival_Yes", target_col)
barplot_func(data, "City", target_col)

# Print the City_map dictionary
print(City_map)

# Create bar plots for relationships between features
barplot_func(data, "Delivery_person_Ratings", "Delivery_person_Age")
barplot_func(data, "multiple_deliveries", "Delivery_person_Age")
barplot_func(data, "multiple_deliveries", "Road_traffic_density")

# Print the Road_traffic_density_map dictionary
print(Road_traffic_density_map)

# Create additional bar plots for relationships between features
barplot_func(data, "Festival_Yes", "Road_traffic_density")
barplot_func(data, "City", "Road_traffic_density")

# Print the City_map dictionary
print(City_map)

# Create bar plots for vehicle-related relationships
barplot_func(data, "Type_of_vehicle", "Vehicle_condition")
barplot_func(data, "Festival_Yes", "multiple_deliveries")
barplot_func(data, "Festival_Yes", "multiple_deliveries")

# Define a function to create custom pair bar plots
custom_pairbarplot <- function(data, columns_to_compare, input_column_type, output_column, type_of_plot) {
  # Grouping and summing each order type based on festival status
  grouped <- data %>% group_by(output_column) %>% summarise(across(columns_to_compare, sum))

  # Plotting the bar plots
  ggplot(grouped, aes_string(x = output_column, y = !columns_to_compare[1])) +
    geom_bar() +
    labs(title = paste0("Comparison of", input_column_type, "for", output_column, "Status"), x = output_column, y = "Sum Count") +
    theme_bw()

  print("\n")
  print(ggplot2::last_plot())
}

# Define variables for first bar plot
columns_to_compare <- c("Type_of_order_Buffet", "Type_of_order_Drinks", "Type_of_order_Meal", "Type_of_order_Snack")
input_column_type <- "Order_Type"
output_column <- "City"
type_of_plot <- "bar"

# Create a custom pair bar plot
custom_pairbarplot(data, columns_to_compare, input_column_type, output_column, type_of_plot)

# Print the City_map dictionary
print(City_map)

# Define variables for second bar plot
columns_to_compare <- c("Type_of_order_Buffet", "Type_of_order_Drinks", "Type_of_order_Meal", "Type_of_order_Snack")
input_column_type <- "Order_Type"  # Update to reflect the actual x-axis label you want
output_column <- "Festival_Yes"
type_of_plot <- "bar"

# Create a custom pair bar plot
custom_pairbarplot(data, columns_to_compare, input_column_type, output_column, type_of_plot)

# Calculate the correlation matrix
correlation_matrix <- cor(data)

# Set up the figure and axes
plot(correlation_matrix, col = colorRamp("coolwarm"), main = "Correlation Heatmap")

# Add a title
text(0, 0.5, "Correlation Matrix", cex = 0.8, adj = c(0.5, 0.5))

# Show the plot
plot(correlation_matrix)

# Get unique values in "City"
unique_cities <- unique(data$City)

# Save the DataFrame as a CSV file
data %>% write_csv("cleaned_data.csv", row.names = FALSE)

#training the model

# Load necessary libraries
library(dplyr)
library(ggplot2)

# Read the data from CSV file
data <- read.csv("data/finalTrain.csv")

# Head of the data
head(data, 3)

# Drop unnecessary columns
data <- data %>%
  select(-ID) %>%
  select(-Delivery_person_ID) %>%
  select(-Time_Orderd) %>%
  select(-Time_Order_picked)

# Head of the modified data
head(data, 3)

# Rename the column "Time_taken (min)" to "Time_taken_min"
data <- rename(data, Time_taken_min = "Time_taken (min)")

# Separate features (X) and target variable (y)
X <- select(data, -Time_taken_min)
y <- select(data, Time_taken_min)

# Head of the features (X)
head(X, 3)

# Head of the target variable (y)
head(y, 3)

# Convert 'Order_Date' column to DateTime format
X$Order_Date <- as.Date(X$Order_Date, format = "%d-%m-%Y")

# Extract Day, Month, and Year from 'Order_Date'
X$Day <- as.day(X$Order_Date)
X$Month <- as.month(X$Order_Date)
X$Year <- as.year(X$Order_Date)

# Drop 'Order_Date' column
X <- X %>% select(-Order_Date)

# Rename columns
X <- rename(X, Day = Day_Ordered, Month = Month_Ordered, Year = Year_Ordered)

# Identify categorical, numerical, and datetime columns
categorical_columns <- colnames(X)[sapply(X, is.character)]
numerical_columns <- colnames(X)[!sapply(X, is.character)]
datetime_columns <- colnames(X)[sapply(X, is.Date)]

# Print categorical, numerical, and datetime columns
cat("Categorical columns:", categorical_columns, "\n")
cat("Numerical columns:", numerical_columns, "\n")
cat("Datetime columns:", datetime_columns, "\n")

# Import necessary libraries for handling missing values, feature scaling, ordinal encoding, and one-hot encoding
library(impute)
library(scales)
library(caret)
library(dplyr)

# Identify categorical columns with missing values
cat_col_null_list <- list()
for (col in categorical_columns) {
  if (sum(is.na(X[[col]])) > 0) {
    cat_col_null_list[[length(cat_col_null_list) + 1]] <- paste(col, "=", sum(is.na(X[[col]])))
  }
}

# Identify numerical columns with missing values
num_col_null_list <- list()
for (col in numerical_columns) {
  if (sum(is.na(X[[col]])) > 0) {
    num_col_null_list[[length(num_col_null_list) + 1]] <- paste(col, "=", sum(is.na(X[[col]])))
  }
}

# Print categorical columns with missing values
cat("Categorical columns with missing values:", cat_col_null_list, "\n")

# Calculate the sum of missing values in categorical columns
null_sum_catcols <- sum(sapply(cat_col_null_list, function(x) { as.numeric(sub("=.*", "", x)) }))
print("Sum of missing values in categorical columns:", null_sum_catcols)

# Print numerical columns with missing values
cat("Numerical columns with missing values:", num_col_null_list, "\n")

# Calculate the sum of missing values in numerical columns
null_sum_numcols <- sum(sapply(num_col_null_list, function(x) { as.numeric(sub("=.*", "", x)) }))
print("Sum of missing values in numerical columns:", null_sum_numcols)

# Calculate the total number of missing values
total_null_values <- null_sum_catcols + null_sum_numcols
print("Total number of missing values:", total_null_values)

# Print the shape of the X dataset
print(dim(X))

# Define custom rankings for each ordinal feature
Weather_conditions_cat <- c("Sunny", "Stormy", "Sandstorms", "Windy", "Cloudy", "Fog", "nan")
Road_traffic_density_cat <- c("Low", "Medium", "High", "Jam", "nan")
City_cat <- c("Urban", "Metropolitan", "Semi-Urban", "nan")

Type_of_order_cat <- c("Buffet", "Drinks", "Meal", "Snack")
Type_of_vehicle_cat <- c("bicycle", "electric_scooter", "motorcycle", "scooter")
Festival_cat <- c("No", "Yes", "nan")

# Identify numerical, categorical ordinal, and categorical one-hot encoded columns
numerical_columns <- c("Delivery_person_Age", "Delivery_person_Ratings", "Restaurant_latitude", "Restaurant_longitude", "Delivery_location_latitude", "Delivery_location_longitude", "Vehicle_condition", "multiple_deliveries", "Day_Ordered", "Month_Ordered", "Year_Ordered")
cat_ordinal_columns <- c("Weather_conditions", "Road_traffic_density", "City")
cat_ohe_columns <- c("Type_of_order", "Type_of_vehicle", "Festival")

# Create pipelines for numerical, categorical ordinal, and categorical one-hot encoded features
num_pipeline <- pipeline(
  steps = list(
    imputer = simpleImputer(strategy = "median"),  # Impute missing values with median
    scaler_numerical = StandardScaler(with_mean = FALSE)  # Standardize and scale numerical features
  )
)

cat_pipeline_ordinal <- pipeline(
  steps = list(
    imputer = simpleImputer(strategy = "most_frequent"),  # Impute missing values with most frequent category
    ordinal_encoder = OrdinalEncoder(categories = list(Weather_conditions_cat, Road_traffic_density_cat, City_cat)),  # Encode ordinal categories
    scaler_ordinal = StandardScaler(with_mean = FALSE)  # Standardize and scale ordinal features
  )
)

cat_pipeline_ohe <- pipeline(
  steps = list(
    imputer = simpleImputer(strategy = "most_frequent"),  # Impute missing values with most frequent category
    onehotencoder = OneHotEncoder(categories = list(Type_of_order_cat, Type_of_vehicle_cat, Festival_cat)),  # One-hot encode categorical features
    scaler_ohe = StandardScaler(with_mean = FALSE)  # Standardize and scale one-hot encoded features
  )
)

# Combine pipelines for numerical, categorical ordinal, and categorical one-hot encoded features using column transformer
pre_processor <- ColumnTransformer(
  transformers = list(
    num_pipeline = num_pipeline, numerical_columns,
    cat_pipeline_ordinal = cat_pipeline_ordinal, cat_ordinal_columns,
    cat_pipeline_ohe = cat_pipeline_ohe, cat_ohe_columns
  ),
  remainder = "passthrough"  # Preserve columns that are not preprocessed
)

# Split the data into training and testing sets
X_train <- X
y_train <- y

X_test <- X
y_test <- y

# Preprocess the training and testing data using the column transformer
X_train_preprocessed <- preprocessor %>% fit_transform(X_train)
X_test_preprocessed <- preprocessor %>% transform(X_test)

# Convert the preprocessed data back to data frames with column names
X_train_preprocessed <- as.data.frame(X_train_preprocessed, columns = preprocessor$get_feature_names_out())
X_test_preprocessed <- as.data.frame(X_test_preprocessed, columns = preprocessor$get_feature_names_out())

# Head of the preprocessed training data
head(X_train_preprocessed, 3)

# Head of the preprocessed testing data
head(X_test_preprocessed, 3)

# Sample 5 rows from the 'cat_pipeline_ohe__Type_of_order' columns
X_train_preprocessed[c("cat_pipeline_ohe__Type_of_order_Snack", "cat_pipeline_ohe__Type_of_order_Buffet", "cat_pipeline_ohe__Type_of_order_Drinks", "cat_pipeline_ohe__Type_of_order_Meal")] %>% sample_n(5)

# Import necessary libraries for linear regression models and evaluation metrics
library(lm)
library(mlbench)

# Create linear regression models
linear_regression <- lm(Time_taken_min ~ ., data = X_train_preprocessed)
ridge_regression <- lmridge(Time_taken_min ~ ., data = X_train_preprocessed)
elastic_net_regression <- glmnet(Time_taken_min ~ ., data = X_train_preprocessed, lambda = 1)
lasso_regression <- glmnet(Time_taken_min ~ ., data = X_train_preprocessed, lambda = 0.001)

# Evaluate the performance of each model on the training data
train_predictions <- predict(linear_regression, X_train_preprocessed)
ridge_train_predictions <- predict(ridge_regression, X_train_preprocessed)
elastic_net_train_predictions <- predict(elastic_net_regression, X_train_preprocessed)
lasso_train_predictions <- predict(lasso_regression, X_train_preprocessed)

train_r2_score <- r2(y_train, train_predictions)
ridge_train_r2_score <- r2(y_train, ridge_train_predictions)
elastic_net_train_r2_score <- r2(y_train, elastic_net_train_predictions)
lasso_train_r2_score <- r2(y_train, lasso_train_predictions)

train_mae_score <- mae(y_train, train_predictions)
ridge_train_mae_score <- mae(y_train, ridge_train_predictions)
elastic_net_train_mae_score <- mae(y_train, elastic_net_train_predictions)
lasso_train_mae_score <- mae(y_train, lasso_train_predictions)

train_mse_score <- mse(y_train, train_predictions)
ridge_train_mse_score <- mse(y_train, ridge_train_predictions)
elastic_net_train_mse_score <- mse(y_train, elastic_net_train_predictions)
lasso_train_mse_score <- mse(y_train, lasso_train_predictions)

# Print the evaluation metrics for each model on the training data
print("Training R^2 Score:")
print(train_r2_score)
print(ridge_train_r2_score)
print(elastic_net_train_r2_score)
print(lasso_train_r2_score)

print("Training MAE Score:")
print(train_mae_score)
print(ridge_train_mae_score)
print(elastic_net_train_mae_score)
print(lasso_train_mae_score)

print("Training MSE Score:")
print(train_mse_score)
print(ridge_train_mse_score)
print(elastic_net_train_mse_score)
print(lasso_train_mse_score)

# Print the coefficients and intercept of the linear regression model
print("Coefficients:")
print(coef(linear_regression))

print("Intercept:")
print(intercept(linear_regression))

# Define a function to evaluate model performance
evaluate_model <- function(true, predicted) {
  mae <- mean(abs(true - predicted))
  mse <- mean((true - predicted)^2)
  rmse <- sqrt(mse)
  r2_square <- r2(true, predicted)
  return(mae, rmse, r2_square)
}

# Create a dictionary of regression models
models <- list(
  LinearRegression = lm(Time_taken_min ~ ., data = X_train_preprocessed),
  Lasso = glmnet(Time_taken_min ~ ., data = X_train_preprocessed, lambda = 0.01),
  Ridge = lmridge(Time_taken_min ~ ., data = X_train_preprocessed),
  ElasticNet = glmnet(Time_taken_min ~ ., data = X_train_preprocessed, lambda = 1)
)

# Initialize empty lists to store model names and R-squared scores
model_list <- list()
r2_list <- list()

# Evaluate each model on the testing data
for (model_name in names(models)) {
  model <- models[[model_name]]

  # Make predictions
  y_pred <- predict(model, X_test_preprocessed)

  # Calculate evaluation metrics
  mae, rmse, r2_square <- evaluate_model(y_test, y_pred)

  # Print model information
  cat(model_name, "\n")
  cat("Model Training Performance\n")
  cat("RMSE: ", rmse, "\n")
  cat("MAE: ", mae, "\n")
  cat("R2 Score: ", round(r2_square * 100, 2), "%\n")
  cat("--------------------------------------------------------------------\n")

  # Append model name and R-squared score to lists
  model_list[[length(model_list) + 1]] <- model_name
  r2_list[[length(r2_list) + 1]] <- r2_square
}

# Print the list of models
print(model_list)

# Print the data information
print(summary(data))
